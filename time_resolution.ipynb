{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Resolution Scan Analysis\n",
    "\n",
    "This notebook use the template created with [template_scan_analysis.ipynb](template_scan_analysis.ipynb). follow initial instructions there and run the notebook till the end.\n",
    "\n",
    "Let us quickly check if the file is really here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls pulse_template.npz\n",
    "!ls /mnt/baobab/sst1m/raw/2018/05/25/SST1M_01/SST1M_01_20180525_*.fits.fz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from digicampipe.io.event_stream import calibration_event_stream\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm #  progress bar\n",
    "from ctapipe.instrument import CameraGeometry\n",
    "from ctapipe.visualization import CameraDisplay\n",
    "from astropy import units as u\n",
    "from scipy.stats.distributions import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the pulse template data and compute the boolean 2D array which indicates which indexes from the template to match camera data for all possibles offset between 0 an 4 ns (1 line per offset, 1 column per sample).\n",
    "We plot the pulse for the different offsets.\n",
    "\n",
    "As we see large offsets cut the template pulse, we pad the template with 0 so it does not happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_pulse = np.load('pulse_template.npz')\n",
    "x = template_pulse['x']\n",
    "delta_x = x[1]-x[0]\n",
    "y = template_pulse['y']\n",
    "dy = template_pulse['dy']\n",
    "print('loaded', template_pulse.keys(), 'from pulse template.')\n",
    "print('template with', len(x), 'points from t={:.4f} ns'.format(x[0]),\n",
    "      'to t={:.4f} ns'.format(x[-1]), 'delta x={:.4f} ns'.format(delta_x))\n",
    "oversampling = int(np.round(4 / delta_x)) #  samples in data are separated by 4ns\n",
    "print('template was oversampled ', oversampling, 'times compared to data.')\n",
    "\n",
    "n_sample = 50\n",
    "max_shift = 4 \n",
    "n_offset = int(max_shift * oversampling)\n",
    "offsets = np.arange(n_offset) * delta_x\n",
    "for idx in range(n_offset):\n",
    "    offsets_idx = np.arange(idx, len(y), oversampling)\n",
    "    plt.plot(y[offsets_idx], '-')\n",
    "    plt.xlabel('sample')\n",
    "    plt.ylabel('normalized amplitude')\n",
    "    plt.title('samples for ' + str(n_offset) + ' offsets ({:.1f} ns)'.format(n_offset * delta_x))\n",
    "\n",
    "plt.figure()\n",
    "x_padded = np.hstack([np.zeros(max_shift*oversampling), x])\n",
    "y_padded = np.hstack([np.zeros(max_shift*oversampling), y])\n",
    "dy_padded = np.hstack([np.ones(max_shift*oversampling)*np.inf, dy])\n",
    "boolean_template_offset = np.zeros([n_offset, len(y_padded)], dtype=bool)\n",
    "for idx in range(n_offset):\n",
    "    offsets_idx = np.arange(idx, len(y_padded), oversampling)\n",
    "    boolean_template_offset[idx, offsets_idx] = True\n",
    "    plt.plot(y_padded[offsets_idx], '-')\n",
    "    plt.xlabel('sample')\n",
    "    plt.ylabel('normalized amplitude')\n",
    "    plt.title('samples for ' + str(n_offset) + ' offsets ({:.1f} ns)'.format(n_offset * delta_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the raw camera files.\n",
    "For each event we normalize the pulse and roughly align it with the template.\n",
    "\n",
    "We then compute the $\\chi^2$ between the measured pulse and the template pulse. We average over all events and store an individual $\\chi^2$ value for each AC LEDs'DAC level, each time offset and each pixel.\n",
    "\n",
    "We store also the mean and the standart deviaton of the integrated pulse over all events for each LED level and each pixel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC_levels = np.arange(0, 750, 5)\n",
    "n_level = len(AC_levels)\n",
    "input_files = sorted(glob(\n",
    "    '/mnt/baobab/sst1m/raw/2018/05/25/SST1M_01/SST1M_01_20180525_*.fits.fz'\n",
    "))\n",
    "print(len(input_files), 'files for', n_level, 'AC levels')\n",
    "assert len(input_files) == n_level, \"ERROR: the number of AC LED levels and the number of files do not match.\"\n",
    "\n",
    "electronic_noise = 0.8 # we take 0.8 LSB as electronic noise\n",
    "zero_pe_integral = 10\n",
    "gain_integral = 22\n",
    "\n",
    "n_pixel = 1296\n",
    "Rough_factor_between_single_pe_amplitude_and_integral = 21 / 5.8\n",
    "chi_squared = np.zeros([n_level, n_pixel, n_offset])\n",
    "mean_integral_for_level = np.zeros([n_level, n_pixel])\n",
    "std_integral_for_level = np.zeros([n_level, n_pixel])\n",
    "all_integral_events = []\n",
    "print()\n",
    "for level_idx, level in enumerate(AC_levels):\n",
    "    print('analyzing file', input_files[level_idx], 'for level', level, 'LSB')\n",
    "    events = calibration_event_stream([input_files[level_idx]])#, max_events=300)\n",
    "    n_events = np.zeros(n_pixel, dtype=np.int)\n",
    "    integral_events = []\n",
    "    for e in events:\n",
    "        adcs = e.data.adc_samples[:, 10:30] - e.data.digicam_baseline[:, None]\n",
    "        integrals = adcs.sum(axis=1)\n",
    "        null_charge = (integrals - zero_pe_integral) / gain_integral < 2.5  #  at least 0.8 pe\n",
    "        n_events += ~null_charge\n",
    "        integral_events.append(integrals)\n",
    "        noise_normalized = np.zeros([n_pixel,])\n",
    "        non_null_adcs_norm = adcs[~null_charge, :] / integrals[~null_charge, None] * Rough_factor_between_single_pe_amplitude_and_integral\n",
    "        non_null_noise_normalized = electronic_noise / integrals[~null_charge] * Rough_factor_between_single_pe_amplitude_and_integral\n",
    "        for idx in range(n_offset):\n",
    "            offsets_idx = np.arange(idx, len(y_padded), oversampling)\n",
    "            template = y_padded[offsets_idx]\n",
    "            squared_template_error = (dy_padded[None, offsets_idx])**2 + (noise_normalized[~null_charge, None])**2\n",
    "            squared_diff_data_template = (non_null_adcs_norm[:, :len(template)] - template[None, :])**2\n",
    "            assert np.any(np.isnan(squared_template_error)) == False\n",
    "            assert np.any(np.isnan(squared_diff_data_template)) == False\n",
    "            assert np.all(squared_template_error > 0)\n",
    "            chi_squared[level_idx, ~null_charge, idx] += np.sum(squared_diff_data_template/squared_template_error, axis=1)/len(template)\n",
    "            #print((integrals[~null_charge] - zero_pe_integral) / gain_integral)\n",
    "            #print(np.sum(~null_charge))\n",
    "            #plt.plot(adcs_normalized.transpose())\n",
    "            #plt.plot(template, 'k-')\n",
    "            #break\n",
    "        #break\n",
    "    #break\n",
    "    chi_squared[level_idx, n_events>0, :] /= n_events[n_events>0, None]\n",
    "    chi_squared[level_idx, n_events==0, :] = np.nan\n",
    "    integral_events = np.array(integral_events)\n",
    "    mean_integral_for_level[level_idx, :] = np.mean(integral_events, axis=0)\n",
    "    std_integral_for_level[level_idx, :] = np.std(integral_events, axis=0)\n",
    "    all_integral_events.append(integral_events)\n",
    "    \n",
    "all_integral_events = np.array(all_integral_events)\n",
    "pe = (mean_integral_for_level - zero_pe_integral) / gain_integral\n",
    "d_pe = (std_integral_for_level - zero_pe_integral) / gain_integral\n",
    "np.savez('chi2.npz', all_integral_events=all_integral_events, pe=pe, d_pe=d_pe, chi_squared=chi_squared)\n",
    "print('done')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the measured charge fluctuations function of the measured charge. We see the fluctuation is proportional to $\\sqrt{p.e.}$ as expected from a Poissonian process until ~700 p.e.\n",
    "\n",
    "We also plot for each pixel the relation between the LEDs AC level and the measured charge. The limit before saturation (700 p.e.) is shown in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(chi2.npz)\n",
    "all_integral_events=data['all_integral_events']\n",
    "pe=data['pe']\n",
    "d_pe=data['d_pe']\n",
    "chi_squared=data['chi_squared']\n",
    "del data\n",
    "\"\"\"\n",
    "events_pe = (all_integral_events - zero_pe_integral) / gain_integral\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(events_pe.flatten(), np.arange(-10.5/22, 10, 1/22))\n",
    "plt.xlabel('charge [p.e.]')\n",
    "plt.ylabel('counts')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "#plt.plot(pe.flatten(), d_pe.flatten() ,'+', label='measured')\n",
    "plt.scatter(pe.flatten(), d_pe.flatten(), 1, np.log10(np.nanmin(chi_squared, axis=2)).flatten(), label='measured')\n",
    "plt.plot(np.logspace(-1, 3, 100), np.sqrt(np.logspace(-1, 3, 100)), 'k--', label='$\\sqrt{p.e.}$')\n",
    "plt.plot([700, 700], [0, np.max(d_pe)], 'r--')\n",
    "plt.xlabel('mean charge measured [p.e.]')\n",
    "plt.ylabel('std charge measured [p.e.]')\n",
    "plt.xscale('log', nonposx='clip')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('$\\log_{10}(\\chi^2)$')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(AC_levels, pe)\n",
    "plt.plot([AC_levels[0], AC_levels[-1]], [700, 700], 'r--')\n",
    "plt.xlabel('DAC level of AC LEDs')\n",
    "plt.ylabel('charge measured [p.e.]')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look for bad pixels by looking pixels where the minimimum $\\chi^2$ is larger than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chi_squared.shape)\n",
    "min_chi_squared_per_pixel = np.nanmin(np.nanmin(chi_squared, axis=0), axis=-1)\n",
    "problematic_pixels = min_chi_squared_per_pixel > 10)\n",
    "best_level_idx = np.argmin(np.min(chi_squared, axis=2), axis=0).astype(np.int)\n",
    "#problematic_pixels = np.logical_or(problematic_pixels, AC_levels[best_level_idx] <= 100)\n",
    "\n",
    "print(np.sum(problematic_pixels), 'problematic pixels:', np.arange(n_pixel)[problematic_pixels])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.semilogy(np.arange(n_pixel)[~problematic_pixels], \n",
    "             min_chi_squared_per_pixel[~problematic_pixels], '+', label='good pixel')\n",
    "plt.semilogy(np.arange(n_pixel)[problematic_pixels],\n",
    "             min_chi_squared_per_pixel[problematic_pixels], 'rx', label='bad pixel')\n",
    "plt.xlabel('pixel #')\n",
    "plt.ylabel('minimum $\\chi^2$')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "#plt.plot(pe[:, ~problematic_pixels].flatten(), d_pe[:, ~problematic_pixels].flatten() ,'+', label='good pixel')\n",
    "plt.scatter(pe[:, ~problematic_pixels].flatten(), d_pe[:, ~problematic_pixels].flatten(), 5, \n",
    "            np.log10(np.nanmin(chi_squared, axis=2))[:, ~problematic_pixels].flatten(), marker='+', label='measured')\n",
    "plt.xscale('log', nonposx='clip')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.xlim(0.1, 1000)\n",
    "plt.ylim(0.1, 50)\n",
    "#plt.clim(0, 100)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('$\\log_{10}(\\chi^2)$')\n",
    "plt.plot(pe[:, problematic_pixels].flatten(), d_pe[:, problematic_pixels].flatten() ,'rx', label='bad pixel') \n",
    "plt.plot(np.arange(0, 1000, 0.1),np.sqrt(np.arange(0, 1000, 0.1)), 'k--', label='$\\sqrt{p.e.}$')\n",
    "plt.xlabel('mean p.e. measured')\n",
    "plt.ylabel('std p.e. measured')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the light level of the CTS was fine, we look for each LED which AC level gave the minimum $\\chi^2$ and check there was some margin. We see some pixels have still their best $\\chi^2$ for the highest light level, probably because of faulty LEDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_level_idx = np.nanargmin(np.nanmin(chi_squared[:, ~problematic_pixels, :], axis=2), axis=0).astype(np.int)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(n_pixel)[~problematic_pixels], AC_levels[best_level_idx], '+')\n",
    "plt.xlabel('pixel #')\n",
    "plt.ylabel('AC level [LSB]')\n",
    "plt.ylim([min(AC_levels)-10, max(AC_levels)+10])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at how the $\\chi^2$ evolves with the time offset. We look for the best light level for each pixel. They are nice parabolas. The offset giving the minimum $\\chi^2$ is the result of the fit. The $1\\sigma$ uncertancy on the offset corresponds to the variation of offset needed to increase $\\chi^2$ by 1. \n",
    "\n",
    "We can get aproximate best offset of each pixel just by taking the offset with the minimum $\\chi^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_max = 10\n",
    "pixels_ok = np.arange(n_pixel)[~problematic_pixels]\n",
    "n_pixel_ok = len(pixels_ok)\n",
    "print(n_pixel_ok, 'good pixels. ploting chi2 for the best level for each of them.')\n",
    "chi_squared_best_offset = np.nanmin(chi_squared[:, ~problematic_pixels, :], axis=2)\n",
    "best_levels_idx = np.nanargmin(chi_squared_best_offset, axis=0).astype(np.int)\n",
    "chi_squared_best_level = np.zeros([n_pixel_ok, n_offset])\n",
    "for pixel_ok, level_idx in enumerate(best_levels_idx):\n",
    "    chi_squared_best_level[pixel_ok, :] = chi_squared[level_idx, pixels_ok[pixel_ok], :]\n",
    "    \n",
    "\n",
    "#print(chi_squared_best_level[2, :])\n",
    "#print(chi_squared[best_level_idx[2], 2, :])\n",
    "pixel_in_fig = np.any(chi_squared_best_level<chi_squared_max, axis=1)\n",
    "level_in_fig = np.any(chi_squared_best_level<chi_squared_max, axis=0)\n",
    "offsets_in_fig = offsets[level_in_fig]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(offsets, chi_squared_best_level.transpose(),'-')\n",
    "plt.xlabel('offset [ns]')\n",
    "plt.ylabel('$\\chi^2 /(N-1)$')\n",
    "plt.xlim([np.min(offsets_in_fig)-.1, np.max(offsets_in_fig)+.1])\n",
    "plt.ylim([0, chi_squared_max])\n",
    "\n",
    "# plot the best offset as the minimum of chi2, depricated in favor or the estimation based on a parabola fit \n",
    "\"\"\"\n",
    "index_best_offsets = np.nanargmin(chi_squared_best_level, axis=-1)\n",
    "print('ploting the best offset for', len(index_best_offsets), 'good pixels')\n",
    "best_offsets_per_pixel = offsets[index_best_offsets]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pixels_ok, best_offsets_per_pixel, '+')\n",
    "plt.xlabel('pixel #')\n",
    "plt.ylabel('best offset [ns]')\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more precise result and error estimation we fit a parabola around the minimum $\\chi^2$ for the best offsets.\n",
    "We plot an histogram of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_rel = np.arange(-6, 7, 1)\n",
    "n_point_fit = len(points_rel)\n",
    "# we create a boolean mask keeping only the points around the min chi2 for each level and each pixel\n",
    "index_best_offsets = np.argmin(chi_squared_best_level, axis=1)\n",
    "index_best_offsets[index_best_offsets < -np.min(points_rel)] = -np.min(points_rel)\n",
    "index_best_offsets[index_best_offsets >= n_offset - np.max(points_rel)] =  n_offset - np.max(points_rel) -1\n",
    "fit_points_bool = np.zeros_like(chi_squared_best_level, dtype=bool)\n",
    "for k in points_rel:\n",
    "    fit_points_bool = np.logical_or(fit_points_bool, np.eye(n_offset,k=k)[index_best_offsets])\n",
    "\n",
    "# we fit parabolas\n",
    "chi2_fitted = chi_squared_best_level[fit_points_bool].reshape(n_pixel_ok, n_point_fit)\n",
    "polys, residuals, rank, singular_values, rcond = np.polyfit(points_rel*delta_x, chi2_fitted.transpose(), 2, full=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "h, xh, _ = plt.hist(residuals, np.linspace(0, 5, 100))\n",
    "plt.xlabel('residuals')\n",
    "plt.ylabel('counts')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are a few fits which failed, we only keep fits with residuals below 5.\n",
    "We calculate the offset and the error form the parameters of the parabola fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_ok = residuals < 5\n",
    "print('pixels with problematic fit:', pixels_ok[~fit_ok])\n",
    "offset_fit_rel = np.ones(n_pixel_ok) * np.nan\n",
    "# offset_fit is the abscice of the minimum of the parabola\n",
    "offset_fit_rel[fit_ok] = -0.5 * polys[1, fit_ok] / polys[0, fit_ok]\n",
    "offset_fit = offset_fit_rel + best_offsets_per_pixel\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pixels_ok, offset_fit, '+')\n",
    "plt.xlabel('pixel #')\n",
    "plt.ylabel('offset [ns]')\n",
    "\n",
    "# d_offset_fit is how much we need to change the offset to increase chi2 by 1 (for 1 sigma confidence level)\n",
    "d_offset_fit = np.ones(n_pixel_ok) * np.nan\n",
    "d_offset_fit[fit_ok] = np.sqrt(1./polys[0, fit_ok]) \n",
    "chi_squared_fit_min = polys[2, :]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.polyval(polys, offset_fit_rel - d_offset_fit) - chi_squared_fit_min, '+',\n",
    "                    label='$\\Delta \\chi^2 (\\Delta t = \\sigma_t)$')\n",
    "plt.plot([0, 1295], [1, 1], 'k--')\n",
    "plt.title('$\\Delta \\chi^2$ at $1 \\sigma$ (should be ~1)')\n",
    "plt.xlabel('pixels #')\n",
    "plt.ylabel('$\\Delta \\chi^2$')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pixels_ok, d_offset_fit, '+')\n",
    "plt.xlabel('pixel #')\n",
    "plt.ylabel('uncertaincy on offset [ns]')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to understand the patern of the measured offset.\n",
    "\n",
    "No obvious correlation with sectors nor boards nor pixel index in boards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = CameraGeometry.from_name(\"DigiCam\")\n",
    "geom.rotate(90 * u.deg)\n",
    "plt.figure(figsize=(12, 10))\n",
    "disp = CameraDisplay(geom)\n",
    "disp.image[pixels_ok] = offset_fit\n",
    "disp.highlight_pixels(problematic_pixels, color='red', linewidth=3)\n",
    "disp.set_limits_minmax(np.min(offset_fit), np.max(offset_fit))\n",
    "disp.add_colorbar()\n",
    "plt.title('measured time offset [ns]')\n",
    "#disp.overlay_pixels_id()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for parterns in the uncertancy of the measured offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "disp = CameraDisplay(geom)\n",
    "disp.image[pixels_ok] = d_offset_fit\n",
    "disp.set_limits_minmax(np.min(d_offset_fit), np.max(d_offset_fit))\n",
    "disp.highlight_pixels(problematic_pixels, color='red', linewidth=3)\n",
    "cbar = disp.add_colorbar()\n",
    "plt.title('uncertaincy on time offset [ns]')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would like the uncertaincy for each pixel function of the charge, we will now fit the offset for all LEDs's AC levels.\n",
    "Then we plot an histogram of the residuals of the parabola fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_rel = np.arange(-6, 7, 1)\n",
    "n_point_fit = len(points_rel)\n",
    "polys = np.ones([3, n_level, n_pixel]) * np.nan\n",
    "residuals = np.ones([n_level, n_pixel]) * np.nan\n",
    "index_offset_rel = np.ones([n_level, n_pixel]) * np.nan\n",
    "for level_idx in range(n_level):\n",
    "    bad_pixels = np.all(np.isnan(chi_squared[level_idx, :, :]), axis=-1)\n",
    "    pixels_ok = np.arange(n_pixel)[~bad_pixels]\n",
    "    print('level', AC_levels[level_idx], ':', len(pixels_ok), '/', n_pixel, 'fits are ok')\n",
    "    chi_squared_level = chi_squared[level_idx, pixels_ok, :]\n",
    "    index_best_offsets = np.nanargmin(chi_squared_level, axis=-1)\n",
    "    index_best_offsets[index_best_offsets < -np.min(points_rel)] = -np.min(points_rel)\n",
    "    index_best_offsets[index_best_offsets >= n_offset - np.max(points_rel)] =  n_offset - np.max(points_rel) -1\n",
    "    fit_points_bool = np.zeros([len(pixels_ok), n_offset], dtype=bool)\n",
    "    for k in points_rel:\n",
    "        fit_points_bool = np.logical_or(fit_points_bool, np.eye(n_offset,k=k)[index_best_offsets])\n",
    "    # we fit parabolas\n",
    "    chi_squared_fit = chi_squared_level[fit_points_bool].reshape(len(pixels_ok), n_point_fit)\n",
    "    polys_level, residuals_level, _, _, _ = np.polyfit(points_rel*delta_x, chi_squared_fit.transpose(), 2, full=True)\n",
    "    polys[:, level_idx, pixels_ok] = polys_level\n",
    "    residuals[level_idx, pixels_ok] = residuals_level\n",
    "    index_offset_rel[level_idx, pixels_ok] = index_best_offsets\n",
    "polys = polys.reshape(3, -1)\n",
    "residuals = residuals.reshape(-1)\n",
    "\n",
    "# we plot the residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "h, xh, _ = plt.hist(residuals[~np.isnan(residuals)], np.logspace(-2, 6, 200))\n",
    "plt.xlabel('residuals')\n",
    "plt.ylabel('counts')\n",
    "plt.xscale('log', nonposx='clip')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "# residuals function of min chi2\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.loglog(residuals, polys[2, :], '+')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the offset and the uncertaincy as previously but for all light levels giving a reasonable fit.\n",
    "Some aditional attention must be taken as we are fitting on less than optimal data.\n",
    "\n",
    "There are for each pixel ~3 functional fits, and they are consistent with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We discard the fits where the residuals are too large.\n",
    "#fit_ok = np.logical_and(residuals < 3, np.min(chi_squared_fit, axis=1) < 10)\n",
    "fit_ok = np.logical_and(residuals < 3, polys[2,:] < 20)\n",
    "print(np.sum(fit_ok), '/', len(fit_ok.flatten()), 'fits succeded')\n",
    "offset_fit_rel = np.ones([n_level* n_pixel]) * np.nan\n",
    "offset_fit_rel[fit_ok] = (-0.5 * polys[1, fit_ok] / polys[0, fit_ok])\n",
    "offset_fit = offset_fit_rel.reshape(n_level, n_pixel) + index_offset_rel * delta_x\n",
    "d_offset_fit = np.ones([n_level* n_pixel]) * np.nan\n",
    "d_offset_fit[fit_ok] = (np.sqrt(1./polys[0, fit_ok]))\n",
    "d_offset_fit = d_offset_fit.reshape(n_level, n_pixel)\n",
    "chi_squared_fit_min = polys[2, :].copy()\n",
    "chi_squared_fit_min[~fit_ok] = np.nan\n",
    "delta_chi_square = np.polyval(polys, offset_fit_rel - d_offset_fit.flatten()) - chi_squared_fit_min\n",
    "\n",
    "\n",
    "# Now we check the hypothesis made to calculate the uncertaincy still holds. When too much off, we discard the fit.\n",
    "fit_ok = np.logical_and(fit_ok, delta_chi_square > 0.8)\n",
    "offset_fit_rel[~fit_ok] = np.nan\n",
    "offset_fit.flatten()[~fit_ok] = np.nan\n",
    "d_offset_fit.flatten()[~fit_ok] = np.nan\n",
    "chi_squared_fit_min[~fit_ok] = np.nan\n",
    "print(np.sum(fit_ok), '/', len(fit_ok), 'fits succeded after further checks')\n",
    "std_offset_fitted = np.nanstd(offset_fit, axis=0)\n",
    "std_offset_fitted_mean = np.nanmean(std_offset_fitted[std_offset_fitted>0])\n",
    "print('the average over all pixels of the deviation of the fitted offset over the AC levels is:', std_offset_fitted)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(len(fit_ok))[fit_ok], delta_chi_square[fit_ok], '+')\n",
    "plt.plot(np.arange(len(fit_ok))[~fit_ok], delta_chi_square[~fit_ok], '+')\n",
    "plt.plot([0, len(fit_ok)], [1, 1], 'k--')\n",
    "plt.title('$\\Delta \\chi^2$ at $1 \\sigma$ (should be ~1)')\n",
    "plt.xlabel('fit #')\n",
    "plt.ylabel('$\\Delta \\chi^2$')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(offset_fit.transpose(), '+')\n",
    "plt.xlabel('pixel #')\n",
    "plt.ylabel('offset [ns]')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(d_offset_fit.transpose(), '+')\n",
    "plt.plot(std_offset_fitted, '.k')\n",
    "plt.xlabel('pixel #')\n",
    "plt.ylabel('error on offset [ns]')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot :\n",
    "- the uncertaincy function of the LEDs AC level\n",
    "- the uncertaincy function of the charge\n",
    "\n",
    "It is very uniform around 0.22 ns in the 20 - 700 p.e. range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(np.ones([n_pixel, 1]) * AC_levels, d_offset_fit, 5, np.log10(pe), '+')\n",
    "plt.xlabel('LED AC level')\n",
    "plt.ylabel('error on offset [ns]')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('$\\log_{10}$(charge [p.e.])')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(pe.flatten()[fit_ok], d_offset_fit.flatten()[fit_ok],5, np.log10(chi_squared_fit_min[fit_ok]), '+')\n",
    "plt.xlim([1, 1000])\n",
    "plt.ylim([0.1, 10])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('$\\log_{10}(\\chi^2)$')\n",
    "plt.xscale('log', nonposx='clip')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.xlabel('measured charge [p.e.]')\n",
    "plt.ylabel('error on offset [ns]')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
